{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtpJf2SRtHbogGa0R9O6Zi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niharika9948/NLP/blob/main/2403A52234_Assignment__8_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Corpus**"
      ],
      "metadata": {
        "id": "P760HdI0JpZ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIg1jhC-ISFZ"
      },
      "outputs": [],
      "source": [
        "D1 = \"I am working as a professor\"\n",
        "\n",
        "D2 = \"I am working in SR University\"\n",
        "\n",
        "D3 = \"I did my PhD in NITW\"\n",
        "\n",
        "D4 = \"I did my masters in OUCE\"\n",
        "\n",
        "D5 = \"I love teaching computer science\"\n",
        "\n",
        "D6 = \"I have published 5 research papers\"\n",
        "\n",
        "D7 = \"I am guiding 3 PhD students\"\n",
        "\n",
        "D8 = \"I attended the AI conference last year\"\n",
        "\n",
        "D9 = \"I enjoy mentoring young researchers\"\n",
        "\n",
        "D10 = \"I am interested in machine learning and data science\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uni Gram** **Counts**"
      ],
      "metadata": {
        "id": "hDdcI-3zJwkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "# Combine the text from D1, D2, D3, D4\n",
        "combined_text = f\"{D1} {D2} {D3} {D4} {D5} {D6} {D7} {D8} {D9} {D10}\"\n",
        "\n",
        "# Tokenize the combined text into words and convert to lowercase\n",
        "words = combined_text.lower().split()\n",
        "\n",
        "# Calculate unigram counts\n",
        "unigram_counts = collections.Counter(words)\n",
        "\n",
        "# Print the unigram counts\n",
        "print(\"Unigram Counts:\")\n",
        "for word, count in unigram_counts.most_common():\n",
        "    print(f\"{word}: {count}\")\n",
        "#Vocabulary size is length of unigrams\n",
        "V=len(unigram_counts)\n",
        "print(\"Vocabulary Size=\",V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuhZbfeIIYkW",
        "outputId": "219114f4-356c-44b1-83b9-bd7db15b18d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Counts:\n",
            "i: 10\n",
            "am: 4\n",
            "in: 4\n",
            "working: 2\n",
            "did: 2\n",
            "my: 2\n",
            "phd: 2\n",
            "science: 2\n",
            "as: 1\n",
            "a: 1\n",
            "professor: 1\n",
            "sr: 1\n",
            "university: 1\n",
            "nitw: 1\n",
            "masters: 1\n",
            "ouce: 1\n",
            "love: 1\n",
            "teaching: 1\n",
            "computer: 1\n",
            "have: 1\n",
            "published: 1\n",
            "5: 1\n",
            "research: 1\n",
            "papers: 1\n",
            "guiding: 1\n",
            "3: 1\n",
            "students: 1\n",
            "attended: 1\n",
            "the: 1\n",
            "ai: 1\n",
            "conference: 1\n",
            "last: 1\n",
            "year: 1\n",
            "enjoy: 1\n",
            "mentoring: 1\n",
            "young: 1\n",
            "researchers: 1\n",
            "interested: 1\n",
            "machine: 1\n",
            "learning: 1\n",
            "and: 1\n",
            "data: 1\n",
            "Vocabulary Size= 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bi-Gram Counts**"
      ],
      "metadata": {
        "id": "7XoyKuu_J2vL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "combined_text = f\"{D1} {D2} {D3} {D4} {D5} {D6} {D7} {D8} {D9} {D10}\"\n",
        "words = combined_text.lower().split()\n",
        "\n",
        "# Generate bigrams\n",
        "bigrams = []\n",
        "for i in range(len(words) - 1):\n",
        "    bigrams.append((words[i], words[i+1]))\n",
        "\n",
        "# Calculate bigram counts\n",
        "bigram_counts = collections.Counter(bigrams)\n",
        "\n",
        "# Print the bigram counts\n",
        "print(\"\\nBigram Counts:\")\n",
        "for bigram, count in bigram_counts.most_common():\n",
        "    print(f\"{bigram[0]} {bigram[1]}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knf0jqNZIpaX",
        "outputId": "8f9f7cb9-4743-43a4-e4d2-22d153fa07bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bigram Counts:\n",
            "i am: 4\n",
            "am working: 2\n",
            "i did: 2\n",
            "did my: 2\n",
            "working as: 1\n",
            "as a: 1\n",
            "a professor: 1\n",
            "professor i: 1\n",
            "working in: 1\n",
            "in sr: 1\n",
            "sr university: 1\n",
            "university i: 1\n",
            "my phd: 1\n",
            "phd in: 1\n",
            "in nitw: 1\n",
            "nitw i: 1\n",
            "my masters: 1\n",
            "masters in: 1\n",
            "in ouce: 1\n",
            "ouce i: 1\n",
            "i love: 1\n",
            "love teaching: 1\n",
            "teaching computer: 1\n",
            "computer science: 1\n",
            "science i: 1\n",
            "i have: 1\n",
            "have published: 1\n",
            "published 5: 1\n",
            "5 research: 1\n",
            "research papers: 1\n",
            "papers i: 1\n",
            "am guiding: 1\n",
            "guiding 3: 1\n",
            "3 phd: 1\n",
            "phd students: 1\n",
            "students i: 1\n",
            "i attended: 1\n",
            "attended the: 1\n",
            "the ai: 1\n",
            "ai conference: 1\n",
            "conference last: 1\n",
            "last year: 1\n",
            "year i: 1\n",
            "i enjoy: 1\n",
            "enjoy mentoring: 1\n",
            "mentoring young: 1\n",
            "young researchers: 1\n",
            "researchers i: 1\n",
            "am interested: 1\n",
            "interested in: 1\n",
            "in machine: 1\n",
            "machine learning: 1\n",
            "learning and: 1\n",
            "and data: 1\n",
            "data science: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tri-Gram Counts**"
      ],
      "metadata": {
        "id": "8qFSID9bJ_So"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "combined_text = f\"{D1} {D2} {D3} {D4} {D5} {D6} {D7} {D8} {D9} {D10}\"\n",
        "words = combined_text.lower().split()\n",
        "\n",
        "# Generate bigrams\n",
        "Trigrams = []\n",
        "for i in range(len(words) - 2):\n",
        "    Trigrams.append((words[i], words[i+1], words[i+2]))\n",
        "\n",
        "# Calculate bigram counts\n",
        "Trigrams_counts = collections.Counter(Trigrams)\n",
        "\n",
        "# Print the bigram counts\n",
        "print(\"\\nTrigrams Counts:\")\n",
        "for Trigrams, count in Trigrams_counts.most_common():\n",
        "    print(f\"{Trigrams[0]} {Trigrams[1]} {Trigrams[2]}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLzPA-6WI13H",
        "outputId": "52b3a00d-bee5-44d5-eb37-8534a260167b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trigrams Counts:\n",
            "i am working: 2\n",
            "i did my: 2\n",
            "am working as: 1\n",
            "working as a: 1\n",
            "as a professor: 1\n",
            "a professor i: 1\n",
            "professor i am: 1\n",
            "am working in: 1\n",
            "working in sr: 1\n",
            "in sr university: 1\n",
            "sr university i: 1\n",
            "university i did: 1\n",
            "did my phd: 1\n",
            "my phd in: 1\n",
            "phd in nitw: 1\n",
            "in nitw i: 1\n",
            "nitw i did: 1\n",
            "did my masters: 1\n",
            "my masters in: 1\n",
            "masters in ouce: 1\n",
            "in ouce i: 1\n",
            "ouce i love: 1\n",
            "i love teaching: 1\n",
            "love teaching computer: 1\n",
            "teaching computer science: 1\n",
            "computer science i: 1\n",
            "science i have: 1\n",
            "i have published: 1\n",
            "have published 5: 1\n",
            "published 5 research: 1\n",
            "5 research papers: 1\n",
            "research papers i: 1\n",
            "papers i am: 1\n",
            "i am guiding: 1\n",
            "am guiding 3: 1\n",
            "guiding 3 phd: 1\n",
            "3 phd students: 1\n",
            "phd students i: 1\n",
            "students i attended: 1\n",
            "i attended the: 1\n",
            "attended the ai: 1\n",
            "the ai conference: 1\n",
            "ai conference last: 1\n",
            "conference last year: 1\n",
            "last year i: 1\n",
            "year i enjoy: 1\n",
            "i enjoy mentoring: 1\n",
            "enjoy mentoring young: 1\n",
            "mentoring young researchers: 1\n",
            "young researchers i: 1\n",
            "researchers i am: 1\n",
            "i am interested: 1\n",
            "am interested in: 1\n",
            "interested in machine: 1\n",
            "in machine learning: 1\n",
            "machine learning and: 1\n",
            "learning and data: 1\n",
            "and data science: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Word Prediction Using Bi-Gram Counts**"
      ],
      "metadata": {
        "id": "oUTMxdqVLKvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_bigram(word_sequence, bigram_counts, unigram_counts):\n",
        "    # Tokenize the input sequence and get the last word\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    last_word = words_in_sequence[-1]\n",
        "\n",
        "    # Find potential next words based on bigrams starting with last_word\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 == last_word:\n",
        "            potential_next_words[w2] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No bigram found starting with '{last_word}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w2 | w1) = Count(w1, w2) / Count(w1)\n",
        "    last_word_unigram_count = unigram_counts.get(last_word, 0)\n",
        "    if last_word_unigram_count == 0:\n",
        "        return f\"'{last_word}' not found in unigram counts. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, bigram_count in potential_next_words.items():\n",
        "        probability = bigram_count / last_word_unigram_count\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts and unigram_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am\"\n",
        "next_word1 = predict_next_word_bigram(sequence1, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_bigram(sequence2, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "\n",
        "sequence3 = \"professor I\"\n",
        "next_word3 = predict_next_word_bigram(sequence3, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence3}', predicted next word: '{next_word3}'\")\n",
        "\n",
        "sequence4 = \"nonexistent word\"\n",
        "next_word4 = predict_next_word_bigram(sequence4, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence4}', predicted next word: '{next_word4}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hsw2ercai9M",
        "outputId": "6f86a46f-0d5a-427d-ee4f-02c9d4104a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  working is  0.5\n",
            "probability of  guiding is  0.25\n",
            "probability of  interested is  0.25\n",
            "Given sequence: 'I am', predicted next word: 'working'\n",
            "probability of  phd is  0.5\n",
            "probability of  masters is  0.5\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n",
            "probability of  am is  0.4\n",
            "probability of  did is  0.2\n",
            "probability of  love is  0.1\n",
            "probability of  have is  0.1\n",
            "probability of  attended is  0.1\n",
            "probability of  enjoy is  0.1\n",
            "Given sequence: 'professor I', predicted next word: 'am'\n",
            "Given sequence: 'nonexistent word', predicted next word: 'No bigram found starting with 'word'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment of Bi-Gram Model**"
      ],
      "metadata": {
        "id": "JC7g5GBNPZ8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_bigram(ip_text, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUFNaxfNcsFC",
        "outputId": "9e9fe000-9c39-4ce5-a1f0-58022f3bbfa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am \n",
            "probability of  working is  0.5\n",
            "probability of  guiding is  0.25\n",
            "probability of  interested is  0.25\n",
            "Given sequence: 'I am ', predicted next word: 'working'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Word Prediction Using Tri-Gram Counts**"
      ],
      "metadata": {
        "id": "quyEJoFKPVyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_trigram(word_sequence, Trigrams_counts, bigram_counts):\n",
        "    # Tokenize the input sequence\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "\n",
        "    # Ensure at least two words for trigram prediction\n",
        "    if len(words_in_sequence) < 2:\n",
        "        return \"Sequence must contain at least two words for trigram prediction.\"\n",
        "\n",
        "    # Get the last two words as a tuple\n",
        "    last_two_words_tuple = tuple(words_in_sequence[-2:])\n",
        "\n",
        "    # Find potential next words based on trigrams starting with the last two words\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2, w3), count in Trigrams_counts.items():\n",
        "        if (w1, w2) == last_two_words_tuple:\n",
        "            potential_next_words[w3] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No trigram found starting with '{' '.join(last_two_words_tuple)}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w3 | w1,w2) = Count(w1, w2, w3) / Count(w1, w2)\n",
        "    # The denominator should be the count of the bigram (w1, w2)\n",
        "    last_two_words_bigram_count = bigram_counts.get(last_two_words_tuple, 0)\n",
        "    if last_two_words_bigram_count == 0:\n",
        "        return f\"'{' '.join(last_two_words_tuple)}' not found as a bigram. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, trigram_count in potential_next_words.items():\n",
        "        probability = trigram_count / last_two_words_bigram_count\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts, unigram_counts, and Trigrams_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am working\"\n",
        "next_word1 = predict_next_word_trigram(sequence1, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_trigram(sequence2, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tun202a5c-nx",
        "outputId": "bd88a063-cb7f-4240-9192-ec6e42d4fa88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  as is  0.5\n",
            "probability of  in is  0.5\n",
            "Given sequence: 'I am working', predicted next word: 'as'\n",
            "probability of  phd is  0.5\n",
            "probability of  masters is  0.5\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment of Tri-Gram Model**"
      ],
      "metadata": {
        "id": "pnZ_1vAkPQI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_trigram(ip_text, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihahuzcyhe7P",
        "outputId": "2268f3e8-8403-405c-8f17-a363c7a517f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am\n",
            "probability of  working is  0.5\n",
            "probability of  guiding is  0.25\n",
            "probability of  interested is  0.25\n",
            "Given sequence: 'I am', predicted next word: 'working'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Word Prediction Using Bi-Gram Counts with Laplace Smoothening**"
      ],
      "metadata": {
        "id": "XfDhrRJWO697"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_bigram_Laplace(word_sequence, bigram_counts, unigram_counts):\n",
        "    # Tokenize the input sequence and get the last word\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    last_word = words_in_sequence[-1]\n",
        "\n",
        "    # Find potential next words based on bigrams starting with last_word\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 == last_word:\n",
        "            potential_next_words[w2] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No bigram found starting with '{last_word}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w2 | w1) = Count(w1, w2) / Count(w1)\n",
        "    last_word_unigram_count = unigram_counts.get(last_word, 0)\n",
        "    if last_word_unigram_count == 0:\n",
        "        return f\"'{last_word}' not found in unigram counts. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, bigram_count in potential_next_words.items():\n",
        "        probability = (bigram_count+1) / (last_word_unigram_count+V)\n",
        "        print(\"probability of \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts and unigram_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am\"\n",
        "next_word1 = predict_next_word_bigram_Laplace(sequence1, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_bigram_Laplace(sequence2, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "\n",
        "sequence3 = \"professor I\"\n",
        "next_word3 = predict_next_word_bigram_Laplace(sequence3, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence3}', predicted next word: '{next_word3}'\")\n",
        "\n",
        "sequence4 = \"nonexistent word\"\n",
        "next_word4 = predict_next_word_bigram_Laplace(sequence4, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence4}', predicted next word: '{next_word4}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UEQ-55Hfkho",
        "outputId": "18620a88-6759-4e01-a11f-3dee94811804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of working is  0.06521739130434782\n",
            "probability of guiding is  0.043478260869565216\n",
            "probability of interested is  0.043478260869565216\n",
            "Given sequence: 'I am', predicted next word: 'working'\n",
            "probability of phd is  0.045454545454545456\n",
            "probability of masters is  0.045454545454545456\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n",
            "probability of am is  0.09615384615384616\n",
            "probability of did is  0.057692307692307696\n",
            "probability of love is  0.038461538461538464\n",
            "probability of have is  0.038461538461538464\n",
            "probability of attended is  0.038461538461538464\n",
            "probability of enjoy is  0.038461538461538464\n",
            "Given sequence: 'professor I', predicted next word: 'am'\n",
            "Given sequence: 'nonexistent word', predicted next word: 'No bigram found starting with 'word'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment of Laplace Smoothening based Bi-Gram Model**"
      ],
      "metadata": {
        "id": "jRNL5v1SOzCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_bigram_Laplace(ip_text, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4BQbeCzhHm3",
        "outputId": "94991b4e-6f23-4b45-c4c3-23dc485aa1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am\n",
            "probability of working is  0.06521739130434782\n",
            "probability of guiding is  0.043478260869565216\n",
            "probability of interested is  0.043478260869565216\n",
            "Given sequence: 'I am', predicted next word: 'working'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Word Prediction Using Tri-Gram Counts based on laplace smoothening**"
      ],
      "metadata": {
        "id": "PLkuY3TvOtZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_trigram_Laplace(word_sequence, Trigrams_counts, bigram_counts):\n",
        "    # Tokenize the input sequence\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "\n",
        "    # Ensure at least two words for trigram prediction\n",
        "    if len(words_in_sequence) < 2:\n",
        "        return \"Sequence must contain at least two words for trigram prediction.\"\n",
        "\n",
        "    # Get the last two words as a tuple\n",
        "    last_two_words_tuple = tuple(words_in_sequence[-2:])\n",
        "\n",
        "    # Find potential next words based on trigrams starting with the last two words\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2, w3), count in Trigrams_counts.items():\n",
        "        if (w1, w2) == last_two_words_tuple:\n",
        "            potential_next_words[w3] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No trigram found starting with '{' '.join(last_two_words_tuple)}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w3 | w1,w2) = Count(w1, w2, w3) / Count(w1, w2)\n",
        "    # The denominator should be the count of the bigram (w1, w2)\n",
        "    last_two_words_bigram_count = bigram_counts.get(last_two_words_tuple, 0)\n",
        "    if last_two_words_bigram_count == 0:\n",
        "        return f\"'{' '.join(last_two_words_tuple)}' not found as a bigram. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, trigram_count in potential_next_words.items():\n",
        "        probability = (trigram_count+1) / (last_two_words_bigram_count+V)\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts, unigram_counts, and Trigrams_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am working\"\n",
        "next_word1 = predict_next_word_trigram_Laplace(sequence1, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_trigram_Laplace(sequence2, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz-Fzd02lYAz",
        "outputId": "a8aaaf45-3286-439f-8127-ed6b7cf2a78d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  as is  0.045454545454545456\n",
            "probability of  in is  0.045454545454545456\n",
            "Given sequence: 'I am working', predicted next word: 'as'\n",
            "probability of  phd is  0.045454545454545456\n",
            "probability of  masters is  0.045454545454545456\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment of Laplace Smoothening based Tri-Gram Model**"
      ],
      "metadata": {
        "id": "Rhh0En7gOkVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_trigram_Laplace(ip_text, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGnCcIN3lrD0",
        "outputId": "7814a3c5-22a5-48e9-e0cd-a036b0466b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am working\n",
            "probability of  as is  0.045454545454545456\n",
            "probability of  in is  0.045454545454545456\n",
            "Given sequence: 'I am working', predicted next word: 'as'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Word Prediction Using Bi-Gram Counts with Add - K Smoothening**"
      ],
      "metadata": {
        "id": "DWR-aUMKOdtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_bigram_K(word_sequence, bigram_counts, unigram_counts, K): #K=0.5-0.01\n",
        "    # Tokenize the input sequence and get the last word\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    last_word = words_in_sequence[-1]\n",
        "\n",
        "    # Find potential next words based on bigrams starting with last_word\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 == last_word:\n",
        "            potential_next_words[w2] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No bigram found starting with '{last_word}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w2 | w1) = Count(w1, w2) / Count(w1)\n",
        "    last_word_unigram_count = unigram_counts.get(last_word, 0)\n",
        "    if last_word_unigram_count == 0:\n",
        "        return f\"'{last_word}' not found in unigram counts. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, bigram_count in potential_next_words.items():\n",
        "        probability = (bigram_count+K) / (last_word_unigram_count+K*V)\n",
        "        print(\"probability of \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts and unigram_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am\"\n",
        "next_word1 = predict_next_word_bigram_K(sequence1, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_bigram_K(sequence2, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "\n",
        "sequence3 = \"professor I\"\n",
        "next_word3 = predict_next_word_bigram_K(sequence3, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence3}', predicted next word: '{next_word3}'\")\n",
        "\n",
        "sequence4 = \"nonexistent word\"\n",
        "next_word4 = predict_next_word_bigram_K(sequence4, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence4}', predicted next word: '{next_word4}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJoBuea9mbFb",
        "outputId": "0884c729-da7f-423a-bde7-317d14f120a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of working is  0.1\n",
            "probability of guiding is  0.06\n",
            "probability of interested is  0.06\n",
            "Given sequence: 'I am', predicted next word: 'working'\n",
            "probability of phd is  0.06521739130434782\n",
            "probability of masters is  0.06521739130434782\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n",
            "probability of am is  0.14516129032258066\n",
            "probability of did is  0.08064516129032258\n",
            "probability of love is  0.04838709677419355\n",
            "probability of have is  0.04838709677419355\n",
            "probability of attended is  0.04838709677419355\n",
            "probability of enjoy is  0.04838709677419355\n",
            "Given sequence: 'professor I', predicted next word: 'am'\n",
            "Given sequence: 'nonexistent word', predicted next word: 'No bigram found starting with 'word'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment of Add-K Smoothening based Bi-Gram Model**"
      ],
      "metadata": {
        "id": "KZeceKsjOSnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_bigram_K(ip_text, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOHJhm_QoZx6",
        "outputId": "9bdc2b38-214f-4167-c552-d02f2455e06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am\n",
            "probability of working is  0.1\n",
            "probability of guiding is  0.06\n",
            "probability of interested is  0.06\n",
            "Given sequence: 'I am', predicted next word: 'working'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Word Prediction Using Tri-Gram Counts with Add - K Smoothening**"
      ],
      "metadata": {
        "id": "F5ftU37BOLX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_trigram_K(word_sequence, Trigrams_counts, bigram_counts,K): #K=0.5-0.01\n",
        "    # Tokenize the input sequence\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "\n",
        "    # Ensure at least two words for trigram prediction\n",
        "    if len(words_in_sequence) < 2:\n",
        "        return \"Sequence must contain at least two words for trigram prediction.\"\n",
        "\n",
        "    # Get the last two words as a tuple\n",
        "    last_two_words_tuple = tuple(words_in_sequence[-2:])\n",
        "\n",
        "    # Find potential next words based on trigrams starting with the last two words\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2, w3), count in Trigrams_counts.items():\n",
        "        if (w1, w2) == last_two_words_tuple:\n",
        "            potential_next_words[w3] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No trigram found starting with '{' '.join(last_two_words_tuple)}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w3 | w1,w2) = Count(w1, w2, w3) / Count(w1, w2)\n",
        "    # The denominator should be the count of the bigram (w1, w2)\n",
        "    last_two_words_bigram_count = bigram_counts.get(last_two_words_tuple, 0)\n",
        "    if last_two_words_bigram_count == 0:\n",
        "        return f\"'{' '.join(last_two_words_tuple)}' not found as a bigram. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, trigram_count in potential_next_words.items():\n",
        "        probability = (trigram_count+K) / (last_two_words_bigram_count+K*V)\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts, unigram_counts, and Trigrams_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am working\"\n",
        "next_word1 = predict_next_word_trigram_K(sequence1, Trigrams_counts, bigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_trigram_K(sequence2, Trigrams_counts, bigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAAZjNF8nTOr",
        "outputId": "590142c7-1345-4aff-aeb1-bd11a12f7174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  as is  0.06521739130434782\n",
            "probability of  in is  0.06521739130434782\n",
            "Given sequence: 'I am working', predicted next word: 'as'\n",
            "probability of  phd is  0.06521739130434782\n",
            "probability of  masters is  0.06521739130434782\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment of Add-K Smoothening based Tri-Gram Model**"
      ],
      "metadata": {
        "id": "o2It6DnROC4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_trigram_K(ip_text, Trigrams_counts, bigram_counts,0.5)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsmIYasxoNtS",
        "outputId": "f9fd15c6-86b5-4821-b00e-06a7e0f9573c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am working\n",
            "probability of  as is  0.06521739130434782\n",
            "probability of  in is  0.06521739130434782\n",
            "Given sequence: 'I am working', predicted next word: 'as'\n"
          ]
        }
      ]
    }
  ]
}